{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import seed\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset,train_perc = 0.8):\n",
    "    data_len = len(dataset)\n",
    "    print(\"length of dataset\"+str(data_len))\n",
    "    train_index = int(data_len*train_perc)\n",
    "    \n",
    "    train = dataset[:train_index,:]\n",
    "    \n",
    "    test = dataset[train_index:,:]\n",
    "    \n",
    "    return(train,test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(activation):\n",
    "    return 1.0/(1.0 + np.exp(-activation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_loss(prediction,actual):\n",
    "    return 0.5*np.sum((actual.T-prediction)*(actual.T-prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(train_X,W1,W2,layer1_output,layer2_output,actual_output):\n",
    "    difference = actual_output.T - layer2_output\n",
    "    delta_output = layer2_output*(1-layer2_output)*difference\n",
    "    delta_hidden = layer1_output*(1-layer1_output)*W2.T.dot(delta_output)\n",
    "    deltaW2 = lr*(delta_output.dot(layer1_output.T)/n_train)\n",
    "    deltaW1 = lr*(delta_hidden.dot(train_X)/n_train)\n",
    "    \n",
    "    return (deltaW1,deltaW2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(train_X,train_Y):\n",
    "    n_input = train_X.shape[1]\n",
    "    W1 = np.random.random((n_hidden,n_input))\n",
    "    W2 = np.random.random((num_classes,n_hidden))\n",
    "    \n",
    "    for epoch in range(n_epoch):\n",
    "        layer1_output = sigmoid(W1.dot(train_X.T))\n",
    "        layer2_output = sigmoid(W2.dot(layer1_output))\n",
    "        \n",
    "        (deltaW1,deltaW2) = back_prop(train_X,W1,W2,layer1_output,layer2_output,train_Y)\n",
    "        \n",
    "        W2 = W2+deltaW2\n",
    "        W1 = W1+deltaW1\n",
    "        \n",
    "        if epoch%100 == 0:\n",
    "            loss = compute_loss(layer2_output,train_Y)\n",
    "            print(str.format('loss in {0}th epoch is {1}',epoch,loss))\n",
    "            \n",
    "    return (W1,W2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(test_X,test_y,params):\n",
    "    (W1,W2) = params\n",
    "    layer1_output = sigmoid(W1.dot(test_X.T))\n",
    "    final = sigmoid(W2.dot(layer1_output))\n",
    "    \n",
    "    prediction = final.argmax(axis = 0)\n",
    "    return np.sum(prediction == test_y)/len(test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_OH(data,num_classes):\n",
    "    \n",
    "    one_hot = np.zeros((len(data),num_classes))  #ask here\n",
    "    \n",
    "    one_hot[np.arange(len(data)),data] = 1    #ask here\n",
    "    print(one_hot)\n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of dataset210\n",
      "ntrain = 168\n",
      "ntest = 42\n",
      "no of classes:4\n",
      "[[1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [1. 0. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [0. 0. 1. 0.]]\n",
      "loss in 0th epoch is 251.9834640316625\n",
      "loss in 100th epoch is 251.98234954269807\n",
      "loss in 200th epoch is 251.98104835587156\n",
      "loss in 300th epoch is 251.97950380348038\n",
      "loss in 400th epoch is 251.97763178243693\n",
      "loss in 500th epoch is 251.97530094536182\n",
      "loss in 600th epoch is 251.97229226661912\n",
      "loss in 700th epoch is 251.9682072237897\n",
      "loss in 800th epoch is 251.96222676371906\n",
      "loss in 900th epoch is 251.95232941773116\n",
      "loss in 1000th epoch is 251.93169266733705\n",
      "loss in 1100th epoch is 251.85264299589966\n",
      "loss in 1200th epoch is 223.3955428837699\n",
      "loss in 1300th epoch is 223.39287233647866\n",
      "loss in 1400th epoch is 223.38938011577434\n",
      "loss in 1500th epoch is 223.38461262319294\n",
      "loss in 1600th epoch is 223.3777040410351\n",
      "loss in 1700th epoch is 223.36676392794988\n",
      "loss in 1800th epoch is 223.34668130313895\n",
      "loss in 1900th epoch is 223.29654797255165\n",
      "Mean Accuracy: 83.333%\n"
     ]
    }
   ],
   "source": [
    "filename = 'train4.csv'\n",
    "\n",
    "df = pd.read_csv(filename,dtype = np.float64,header = None)\n",
    "dataset = np.array(df)\n",
    "\n",
    "(train,test) = split_dataset(dataset)\n",
    "n_train = len(train)\n",
    "print('ntrain = '+str(n_train))\n",
    "n_test = len(test)\n",
    "print('ntest = '+str(n_test))\n",
    "\n",
    "lr = 1\n",
    "n_epoch = 2000\n",
    "\n",
    "num_classes = len(np.unique(dataset[:,-1]))\n",
    "print('no of classes:'+str(num_classes))\n",
    "train_one_hot = convert_to_OH(train[:,-1].astype(int),num_classes)  #ask here\n",
    "\n",
    "n_hidden = 20\n",
    "\n",
    "params = train_network(train[:,:-1],train_one_hot)\n",
    "accuracy = evaluate(test[:,:-1],test[:,-1],params)*100\n",
    "print('Mean Accuracy: %.3f%%'%accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
